"""ChromaDB storage module for TokenKeeper.

Provides persistent vector storage using ChromaDB, including client/collection
management, chunk ID generation with POSIX path normalisation, file-level
chunk replacement with empty-delete guard, content hashing for change detection,
and reindex-needed checks.

Exports:
    create_chroma_client   -- Create a PersistentClient at a given path
    get_or_create_collection -- Get or create a named collection with cosine space
    make_chunk_id          -- Generate deterministic chunk ID from file + index
    get_source_file_from_id -- Extract source file path from a chunk ID
    get_chunk_ids_for_file -- List all chunk IDs for a given source file
    replace_file_chunks    -- Atomically replace all chunks for a file
    compute_file_hash      -- SHA-256 hex digest of content
    get_stored_hash        -- Retrieve stored content hash for a file
    file_needs_reindex     -- Check if file content has changed since last index
"""

from __future__ import annotations

import hashlib
import logging
from pathlib import Path, PurePosixPath

import chromadb

from tokenkeeper.ingestion import DocumentChunk

logger = logging.getLogger("tokenkeeper.storage")


# ---------------------------------------------------------------------------
# Client and collection management
# ---------------------------------------------------------------------------


def create_chroma_client(chroma_path: str | Path) -> chromadb.ClientAPI:
    """Create a ChromaDB PersistentClient at the given path.

    Creates the directory if it does not exist.

    Args:
        chroma_path: Filesystem path for persistent storage.

    Returns:
        A ``chromadb.PersistentClient`` instance.
    """
    resolved = Path(chroma_path).resolve()
    resolved.mkdir(parents=True, exist_ok=True)
    logger.debug("Creating ChromaDB PersistentClient at %s", resolved)
    return chromadb.PersistentClient(path=str(resolved))


def get_or_create_collection(
    client: chromadb.ClientAPI,
    name: str = "documents",
) -> chromadb.Collection:
    """Get or create a named collection with cosine distance metric.

    Args:
        client: A ChromaDB client instance.
        name:   Collection name (default ``"documents"``).

    Returns:
        A ``chromadb.Collection`` configured for cosine similarity.
    """
    return client.get_or_create_collection(
        name=name,
        metadata={"hnsw:space": "cosine"},
    )


# ---------------------------------------------------------------------------
# Chunk ID utilities
# ---------------------------------------------------------------------------


def make_chunk_id(source_file: str, chunk_index: int) -> str:
    """Generate a deterministic chunk ID from source file and index.

    Converts backslashes to POSIX forward slashes for cross-platform
    consistency.

    Args:
        source_file: Relative path to the source document.
        chunk_index: 0-based chunk position within the document.

    Returns:
        A string like ``"docs/readme.md::chunk_0"``.
    """
    posix_path = source_file.replace("\\", "/")
    return f"{posix_path}::chunk_{chunk_index}"


def get_source_file_from_id(chunk_id: str) -> str:
    """Extract the source file path from a chunk ID.

    Args:
        chunk_id: A chunk ID generated by :func:`make_chunk_id`.

    Returns:
        The source file path portion (e.g. ``"docs/readme.md"``).
    """
    return chunk_id.split("::chunk_")[0]


# ---------------------------------------------------------------------------
# Collection queries
# ---------------------------------------------------------------------------


def get_chunk_ids_for_file(
    collection: chromadb.Collection,
    source_file: str,
) -> list[str]:
    """Find all chunk IDs for a given source file in the collection.

    Args:
        collection:  A ChromaDB collection.
        source_file: Relative path to the source document.

    Returns:
        List of chunk IDs belonging to the file (may be empty).
    """
    posix_path = PurePosixPath(source_file).as_posix()
    result = collection.get(
        where={"source_file": posix_path},
        include=[],
    )
    return result["ids"]


# ---------------------------------------------------------------------------
# File chunk replacement
# ---------------------------------------------------------------------------


def replace_file_chunks(
    collection: chromadb.Collection,
    source_file: str,
    chunks: list[DocumentChunk],
    embeddings: list[list[float]],
    file_hash: str = "",
    bm25_tokens_list: list[str] | None = None,
    indexed_at: float | None = None,
) -> int:
    """Atomically replace all chunks for a file in the collection.

    Steps:
    1. Find existing chunk IDs for the file.
    2. Delete them **only if there are any** (empty-delete guard).
    3. Add the new chunks with metadata and embeddings.

    CRITICAL: The ``if stale_ids:`` guard prevents a ChromaDB bug where
    calling ``collection.delete(ids=[])`` removes ALL documents.

    Args:
        collection:       A ChromaDB collection.
        source_file:      Relative path to the source document.
        chunks:           List of :class:`DocumentChunk` instances.
        embeddings:       Corresponding embedding vectors.
        file_hash:        Pre-computed file-level content hash. If empty,
                          falls back to hashing each chunk's content.
        bm25_tokens_list: Pre-computed BM25 token strings per chunk.
                          If None, bm25_tokens metadata is omitted.
        indexed_at:       Timestamp (``time.time()``) when the file was
                          indexed. If None, the field is omitted from
                          metadata.

    Returns:
        Number of chunks stored.
    """
    posix_path = PurePosixPath(source_file).as_posix()

    # Step 1: Find and remove stale chunks
    stale_ids = get_chunk_ids_for_file(collection, posix_path)
    if stale_ids:
        collection.delete(ids=stale_ids)
        logger.debug("Deleted %d stale chunks for %s", len(stale_ids), posix_path)

    # Step 2: Build new chunk data
    ids = [make_chunk_id(posix_path, i) for i, _ in enumerate(chunks)]
    documents = [chunk.content for chunk in chunks]
    metadatas = []
    for i, chunk in enumerate(chunks):
        fm = chunk.frontmatter or {}
        title = fm.get("title", "")
        tags_raw = fm.get("tags", [])
        tags_str = ",".join(tags_raw) if isinstance(tags_raw, list) else str(tags_raw)
        meta: dict[str, str | int | float | bool] = {
            "source_file": posix_path,
            "chunk_index": i,
            "title": title or "",
            "tags": tags_str,
            "content_hash": file_hash if file_hash else compute_file_hash(chunk.content),
        }
        if bm25_tokens_list is not None and i < len(bm25_tokens_list):
            meta["bm25_tokens"] = bm25_tokens_list[i]
        if indexed_at is not None:
            meta["indexed_at"] = indexed_at
        if chunk.heading_hierarchy:
            meta["heading_hierarchy"] = chunk.heading_hierarchy
        if chunk.language:
            meta["language"] = chunk.language
        if chunk.symbol_name:
            meta["symbol_name"] = chunk.symbol_name
        if chunk.symbol_type:
            meta["symbol_type"] = chunk.symbol_type
        if chunk.line_start:
            meta["line_start"] = chunk.line_start
        if chunk.line_end:
            meta["line_end"] = chunk.line_end
        metadatas.append(meta)

    # Step 3: Add new chunks
    collection.add(
        ids=ids,
        documents=documents,
        metadatas=metadatas,
        embeddings=embeddings,
    )
    logger.info("Stored %d chunks for %s", len(chunks), posix_path)
    return len(chunks)


# ---------------------------------------------------------------------------
# Content hashing
# ---------------------------------------------------------------------------


def compute_file_hash(content: str) -> str:
    """Compute SHA-256 hex digest of content.

    Args:
        content: The text content to hash.

    Returns:
        A 64-character lowercase hexadecimal string.
    """
    return hashlib.sha256(content.encode("utf-8")).hexdigest()


def get_stored_hash(
    collection: chromadb.Collection,
    source_file: str,
) -> str | None:
    """Retrieve the stored content hash for the first chunk of a file.

    Args:
        collection:  A ChromaDB collection.
        source_file: Relative path to the source document.

    Returns:
        The ``content_hash`` metadata value, or ``None`` if the file
        has no chunks in the collection.
    """
    posix_path = PurePosixPath(source_file).as_posix()
    result = collection.get(
        where={"source_file": posix_path},
        include=["metadatas"],
        limit=1,
    )
    if result["ids"]:
        return result["metadatas"][0].get("content_hash")
    return None


def file_needs_reindex(
    collection: chromadb.Collection,
    source_file: str,
    current_content: str,
) -> bool:
    """Check whether a file needs re-indexing based on content hash.

    Compares the SHA-256 hash of *current_content* against the stored
    hash in the collection. Returns ``True`` if the hashes differ or
    if the file has never been indexed.

    Args:
        collection:      A ChromaDB collection.
        source_file:     Relative path to the source document.
        current_content: The current text content of the file.

    Returns:
        ``True`` if the file should be re-indexed, ``False`` otherwise.
    """
    stored = get_stored_hash(collection, source_file)
    if stored is None:
        return True
    return stored != compute_file_hash(current_content)
